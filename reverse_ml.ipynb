{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input, Embedding, Dense, Concatenate, Flatten, Dropout,BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132296/1402317043.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  output_values[\"First Layer\"] = label_encoder.fit_transform(output_values[\"First Layer\"])\n",
      "/tmp/ipykernel_132296/1402317043.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  output_values[\"Second Layer\"] = label_encoder.transform(output_values[\"Second Layer\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = data.copy()\n",
    "\n",
    "feature_headings = [\n",
    "    \"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"First Layer\",\"Second Layer\"\n",
    "]\n",
    "\n",
    "output_values = labels[feature_headings]\n",
    "input_features = labels[[c for c in labels.columns if c not in feature_headings]]\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "output_values[\"First Layer\"] = label_encoder.fit_transform(output_values[\"First Layer\"])\n",
    "output_values[\"Second Layer\"] = label_encoder.transform(output_values[\"Second Layer\"])\n",
    "\n",
    "unique_materials = pd.unique(data[['First Layer', 'Second Layer']].values.ravel())\n",
    "\n",
    "\n",
    "num_materials = len(unique_materials)\n",
    "num_wavelengths = 351\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(input_features, output_values, test_size=0.2, random_state=42)\n",
    "input_train, input_val, output_train, output_val = train_test_split(input_train, output_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(num_wavelengths,)\n",
    "\n",
    "ndl1 = BatchNormalization()(i)\n",
    "dl1 = Dense(num_wavelengths, activation=\"PReLU\", input_shape=(num_wavelengths,))(ndl1)\n",
    "\n",
    "m1dl2 = Dense(256, activation=\"PReLU\")(dl1)\n",
    "m2dl2 = Dense(256, activation=\"PReLU\")(dl1)\n",
    "tdl2 = Dense(256, activation=\"PReLU\")(ndl1)\n",
    "\n",
    "concatenated_input = Concatenate()([m1dl2, m2dl2, tdl2])\n",
    "\n",
    "common_layer = Dense(64)(concatenated_input)\n",
    "\n",
    "m1dl3 = Dense(256, activation=\"PReLU\")(common_layer)\n",
    "m2dl3 = Dense(256, activation=\"PReLU\")(common_layer)\n",
    "tdl3 = Dense(256, activation=\"PReLU\")(common_layer)\n",
    "\n",
    "out1 = Dense(num_materials, \"softmax\", name=\"first_layer\")(m1dl3)\n",
    "out2 = Dense(num_materials, \"softmax\", name=\"second_layer\")(m2dl3)\n",
    "thickenesses = Dense(6, \"sigmoid\", name=\"thicknesses\")(tdl3)\n",
    "\n",
    "classifier = Model(inputs=i, outputs=[out1,out2,thickenesses])\n",
    "classifier.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\n",
    "        \"first_layer\": keras.losses.SparseCategoricalCrossentropy(),\n",
    "        \"second_layer\": keras.losses.SparseCategoricalCrossentropy(),\n",
    "        \"thicknesses\": \"mean_squared_error\"\n",
    "    },\n",
    "    metrics={\n",
    "        \"first_layer\": tf.metrics.SparseCategoricalAccuracy(\"acc1\"),\n",
    "        \"second_layer\": tf.metrics.SparseCategoricalAccuracy(\"acc2\"),\n",
    "        \"thicknesses\": \"accuracy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 5s 6ms/step - loss: 1459.1547 - first_layer_loss: 2.9512 - second_layer_loss: 3.2199 - thicknesses_loss: 1452.9834 - first_layer_acc1: 0.1566 - second_layer_acc2: 0.1080 - thicknesses_accuracy: 0.2034 - val_loss: 1440.0433 - val_first_layer_loss: 2.5680 - val_second_layer_loss: 3.0083 - val_thicknesses_loss: 1434.4668 - val_first_layer_acc1: 0.2456 - val_second_layer_acc2: 0.1344 - val_thicknesses_accuracy: 0.1858\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1458.5255 - first_layer_loss: 2.6431 - second_layer_loss: 3.0349 - thicknesses_loss: 1452.8473 - first_layer_acc1: 0.2082 - second_layer_acc2: 0.1383 - thicknesses_accuracy: 0.2278 - val_loss: 1439.8402 - val_first_layer_loss: 2.4419 - val_second_layer_loss: 2.9311 - val_thicknesses_loss: 1434.4667 - val_first_layer_acc1: 0.2942 - val_second_layer_acc2: 0.1627 - val_thicknesses_accuracy: 0.2477\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1458.3853 - first_layer_loss: 2.5541 - second_layer_loss: 2.9827 - thicknesses_loss: 1452.8485 - first_layer_acc1: 0.2266 - second_layer_acc2: 0.1498 - thicknesses_accuracy: 0.2426 - val_loss: 1439.8505 - val_first_layer_loss: 2.4609 - val_second_layer_loss: 2.9225 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2529 - val_second_layer_acc2: 0.1715 - val_thicknesses_accuracy: 0.2325\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 1458.3579 - first_layer_loss: 2.5505 - second_layer_loss: 2.9592 - thicknesses_loss: 1452.8484 - first_layer_acc1: 0.2318 - second_layer_acc2: 0.1557 - thicknesses_accuracy: 0.2481 - val_loss: 1439.7098 - val_first_layer_loss: 2.3802 - val_second_layer_loss: 2.8633 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2912 - val_second_layer_acc2: 0.1723 - val_thicknesses_accuracy: 0.2594\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1458.2720 - first_layer_loss: 2.4923 - second_layer_loss: 2.9322 - thicknesses_loss: 1452.8484 - first_layer_acc1: 0.2418 - second_layer_acc2: 0.1590 - thicknesses_accuracy: 0.2543 - val_loss: 1439.7279 - val_first_layer_loss: 2.3866 - val_second_layer_loss: 2.8742 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2723 - val_second_layer_acc2: 0.1819 - val_thicknesses_accuracy: 0.2500\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 1458.2404 - first_layer_loss: 2.4780 - second_layer_loss: 2.9137 - thicknesses_loss: 1452.8478 - first_layer_acc1: 0.2458 - second_layer_acc2: 0.1615 - thicknesses_accuracy: 0.2550 - val_loss: 1439.6715 - val_first_layer_loss: 2.3375 - val_second_layer_loss: 2.8675 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2752 - val_second_layer_acc2: 0.1887 - val_thicknesses_accuracy: 0.2575\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 1458.2206 - first_layer_loss: 2.4688 - second_layer_loss: 2.9046 - thicknesses_loss: 1452.8477 - first_layer_acc1: 0.2423 - second_layer_acc2: 0.1672 - thicknesses_accuracy: 0.2563 - val_loss: 1439.6039 - val_first_layer_loss: 2.3231 - val_second_layer_loss: 2.8145 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2775 - val_second_layer_acc2: 0.1950 - val_thicknesses_accuracy: 0.2542\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1458.1799 - first_layer_loss: 2.4439 - second_layer_loss: 2.8880 - thicknesses_loss: 1452.8483 - first_layer_acc1: 0.2524 - second_layer_acc2: 0.1710 - thicknesses_accuracy: 0.2589 - val_loss: 1439.5895 - val_first_layer_loss: 2.3193 - val_second_layer_loss: 2.8031 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.2958 - val_second_layer_acc2: 0.1971 - val_thicknesses_accuracy: 0.2569\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1458.1487 - first_layer_loss: 2.4246 - second_layer_loss: 2.8760 - thicknesses_loss: 1452.8477 - first_layer_acc1: 0.2542 - second_layer_acc2: 0.1731 - thicknesses_accuracy: 0.2569 - val_loss: 1439.5352 - val_first_layer_loss: 2.2768 - val_second_layer_loss: 2.7919 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.3027 - val_second_layer_acc2: 0.1902 - val_thicknesses_accuracy: 0.2460\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 1458.1173 - first_layer_loss: 2.4082 - second_layer_loss: 2.8609 - thicknesses_loss: 1452.8473 - first_layer_acc1: 0.2550 - second_layer_acc2: 0.1727 - thicknesses_accuracy: 0.2590 - val_loss: 1439.5549 - val_first_layer_loss: 2.2737 - val_second_layer_loss: 2.8143 - val_thicknesses_loss: 1434.4666 - val_first_layer_acc1: 0.3017 - val_second_layer_acc2: 0.1915 - val_thicknesses_accuracy: 0.2544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f171c1334c0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "    input_train,\n",
    "    {\n",
    "        \"first_layer\": output_train[\"First Layer\"],\n",
    "        \"second_layer\": output_train[\"Second Layer\"],\n",
    "        \"thicknesses\": output_train[[\"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\"]]\n",
    "    },\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        input_val,\n",
    "        {\n",
    "            \"first_layer\": output_val[\"First Layer\"],\n",
    "            \"second_layer\": output_val[\"Second Layer\"],\n",
    "            \"thicknesses\": output_val[[\"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\"]]\n",
    "        }\n",
    "    ),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "modelpredicted: ['Mn' 'Mg']\n",
      "actual combination: ['Mg' 'KTaO3']\n",
      "predicted_thicknesses: [1. 1. 1. 1. 1. 1.]\n",
      "actual thicknesses: [60 60 30 20 60 40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clk/repos/l2-transfer-matrix-method/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(input_test) - 1)\n",
    "input_sample = input_test.iloc[[index]]\n",
    "output_sample = output_test.iloc[[index]]\n",
    "mat_predict = classifier.predict([input_sample])\n",
    "pred = label_encoder.inverse_transform([np.argmax(mat_predict[0]), np.argmax(mat_predict[1])])\n",
    "act = label_encoder.inverse_transform([output_sample[\"First Layer\"], output_sample[\"Second Layer\"]])\n",
    "\n",
    "print(f\"modelpredicted: {pred}\")\n",
    "print(f\"actual combination: {act}\")\n",
    "\n",
    "print(f\"predicted_thicknesses: {mat_predict[2][0]}\")\n",
    "print(f\"actual thicknesses: {output_sample.values[0][:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7.2226249e-02, 4.9381154e-03, 2.4150813e-05, 2.1980939e-02,\n",
       "         1.3405672e-05, 1.6721595e-02, 1.1273656e-05, 7.4959712e-06,\n",
       "         1.2992452e-02, 2.2793314e-04, 1.8634672e-01, 1.4656462e-02,\n",
       "         5.4424326e-03, 2.2721305e-01, 2.2199424e-01, 6.9313813e-03,\n",
       "         6.0199723e-03, 2.6527300e-06, 1.6992679e-02, 1.2730055e-02,\n",
       "         3.1961221e-02, 1.6137188e-02, 2.3986079e-02, 8.3818544e-05,\n",
       "         5.7677142e-03, 3.4168903e-02, 1.6079513e-04, 1.9238483e-03,\n",
       "         7.0986394e-03, 4.5850862e-02, 1.4524620e-03, 3.9352193e-03]],\n",
       "       dtype=float32),\n",
       " array([[0.05476758, 0.02387645, 0.01805017, 0.02706322, 0.02891464,\n",
       "         0.03094666, 0.02684937, 0.01894384, 0.03261182, 0.02300763,\n",
       "         0.07156663, 0.03360134, 0.02771512, 0.07059391, 0.0704021 ,\n",
       "         0.02462649, 0.02410819, 0.0257853 , 0.03462988, 0.02816422,\n",
       "         0.02745189, 0.02518246, 0.0295989 , 0.02630394, 0.02285752,\n",
       "         0.02779222, 0.02063611, 0.01990551, 0.02404341, 0.03585097,\n",
       "         0.02182276, 0.02232978]], dtype=float32),\n",
       " array([[1., 1., 1., 1., 1., 1.]], dtype=float32)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
