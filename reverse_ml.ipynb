{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147414/1402317043.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  output_values[\"First Layer\"] = label_encoder.fit_transform(output_values[\"First Layer\"])\n",
      "/tmp/ipykernel_147414/1402317043.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  output_values[\"Second Layer\"] = label_encoder.transform(output_values[\"Second Layer\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = data.copy()\n",
    "\n",
    "feature_headings = [\n",
    "    \"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"First Layer\",\"Second Layer\"\n",
    "]\n",
    "\n",
    "output_values = labels[feature_headings]\n",
    "input_features = labels[[c for c in labels.columns if c not in feature_headings]]\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "output_values[\"First Layer\"] = label_encoder.fit_transform(output_values[\"First Layer\"])\n",
    "output_values[\"Second Layer\"] = label_encoder.transform(output_values[\"Second Layer\"])\n",
    "\n",
    "unique_materials = pd.unique(data[['First Layer', 'Second Layer']].values.ravel())\n",
    "\n",
    "\n",
    "num_materials = len(unique_materials)\n",
    "num_wavelengths = 351\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(input_features, output_values, test_size=0.2, random_state=42)\n",
    "input_train, input_val, output_train, output_val = train_test_split(input_train, output_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(num_wavelengths,)\n",
    "\n",
    "dl1 = Dense(num_wavelengths, activation=\"PReLU\", input_shape=(num_wavelengths,))(i)\n",
    "\n",
    "m1dl2 = Dense(256, activation=\"PReLU\")(dl1)\n",
    "m2dl2 = Dense(256, activation=\"PReLU\")(dl1)\n",
    "tdl2 = Dense(256, activation=\"PReLU\")(dl1)\n",
    "\n",
    "concatenated_input = Concatenate()([m1dl2, m2dl2, tdl2])\n",
    "\n",
    "common_layer = Dense(500)(concatenated_input)\n",
    "\n",
    "m1dl3 = Dense(128, activation=\"PReLU\")(common_layer)\n",
    "m2dl3 = Dense(128, activation=\"PReLU\")(common_layer)\n",
    "tdl3 = Dense(128, activation=\"PReLU\")(common_layer)\n",
    "\n",
    "out1 = Dense(num_materials, \"softmax\", name=\"first_layer\")(m1dl3)\n",
    "out2 = Dense(num_materials, \"softmax\", name=\"second_layer\")(m2dl3)\n",
    "\n",
    "material1 = Concatenate()([tdl3,out1])\n",
    "material2 = Concatenate()([tdl3,out2])\n",
    "t1 = Dense(1, \"relu\", name=\"t1\")(material1)\n",
    "t2 = Dense(1, \"relu\", name=\"t2\")(material2)\n",
    "t3 = Dense(1, \"relu\", name=\"t3\")(material1)\n",
    "t4 = Dense(1, \"relu\", name=\"t4\")(material2)\n",
    "t5 = Dense(1, \"relu\", name=\"t5\")(material1)\n",
    "t6 = Dense(1, \"relu\", name=\"t6\")(material2)\n",
    "\n",
    "classifier = Model(inputs=i, outputs=[out1,out2,t1,t2,t3,t4,t5,t6])\n",
    "classifier.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\n",
    "        \"first_layer\": keras.losses.SparseCategoricalCrossentropy(),\n",
    "        \"second_layer\": keras.losses.SparseCategoricalCrossentropy(),\n",
    "        \"t1\": \"mean_squared_error\",\n",
    "        \"t2\": \"mean_squared_error\",\n",
    "        \"t3\": \"mean_squared_error\",\n",
    "        \"t4\": \"mean_squared_error\",\n",
    "        \"t5\": \"mean_squared_error\",\n",
    "        \"t6\": \"mean_squared_error\",\n",
    "    },\n",
    "    metrics={\n",
    "        \"first_layer\": tf.metrics.SparseCategoricalAccuracy(\"acc1\"),\n",
    "        \"second_layer\": tf.metrics.SparseCategoricalAccuracy(\"acc2\"),\n",
    "        \"t1\": \"accuracy\",\n",
    "        \"t2\": \"accuracy\",\n",
    "        \"t3\": \"accuracy\",\n",
    "        \"t4\": \"accuracy\",\n",
    "        \"t5\": \"accuracy\",\n",
    "        \"t6\": \"accuracy\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 10s 10ms/step - loss: 3158.5588 - first_layer_loss: 3.6195 - second_layer_loss: 3.6788 - t1_loss: 329.7116 - t2_loss: 325.2240 - t3_loss: 328.8613 - t4_loss: 328.8975 - t5_loss: 1512.0052 - t6_loss: 326.5611 - first_layer_acc1: 0.0455 - second_layer_acc2: 0.0307 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2955.2231 - val_first_layer_loss: 3.4481 - val_second_layer_loss: 3.4650 - val_t1_loss: 283.1953 - val_t2_loss: 285.1455 - val_t3_loss: 289.4074 - val_t4_loss: 284.3921 - val_t5_loss: 1513.7084 - val_t6_loss: 292.4618 - val_first_layer_acc1: 0.0500 - val_second_layer_acc2: 0.0356 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2942.7251 - first_layer_loss: 3.4250 - second_layer_loss: 3.4077 - t1_loss: 287.1051 - t2_loss: 281.5148 - t3_loss: 283.4214 - t4_loss: 284.5714 - t5_loss: 1512.0052 - t6_loss: 287.2733 - first_layer_acc1: 0.0473 - second_layer_acc2: 0.0426 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2921.5959 - val_first_layer_loss: 3.3836 - val_second_layer_loss: 3.3689 - val_t1_loss: 277.5347 - val_t2_loss: 279.4469 - val_t3_loss: 282.3750 - val_t4_loss: 278.7521 - val_t5_loss: 1513.7084 - val_t6_loss: 283.0265 - val_first_layer_acc1: 0.0446 - val_second_layer_acc2: 0.0571 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2930.8948 - first_layer_loss: 3.2990 - second_layer_loss: 3.3127 - t1_loss: 284.6208 - t2_loss: 279.1468 - t3_loss: 280.4262 - t4_loss: 282.8635 - t5_loss: 1512.0052 - t6_loss: 285.2219 - first_layer_acc1: 0.0497 - second_layer_acc2: 0.0542 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2919.3328 - val_first_layer_loss: 3.2154 - val_second_layer_loss: 3.3586 - val_t1_loss: 278.0235 - val_t2_loss: 279.7869 - val_t3_loss: 282.9197 - val_t4_loss: 279.1439 - val_t5_loss: 1513.7084 - val_t6_loss: 279.1763 - val_first_layer_acc1: 0.0613 - val_second_layer_acc2: 0.0519 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2929.0200 - first_layer_loss: 3.2339 - second_layer_loss: 3.2710 - t1_loss: 283.5273 - t2_loss: 279.5296 - t3_loss: 280.6165 - t4_loss: 282.3788 - t5_loss: 1512.0052 - t6_loss: 284.4564 - first_layer_acc1: 0.0596 - second_layer_acc2: 0.0654 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2910.2046 - val_first_layer_loss: 3.1976 - val_second_layer_loss: 3.2190 - val_t1_loss: 276.6339 - val_t2_loss: 278.5370 - val_t3_loss: 279.5236 - val_t4_loss: 274.3571 - val_t5_loss: 1513.7084 - val_t6_loss: 281.0278 - val_first_layer_acc1: 0.0562 - val_second_layer_acc2: 0.0952 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2927.1101 - first_layer_loss: 3.1818 - second_layer_loss: 3.2336 - t1_loss: 282.5241 - t2_loss: 281.0287 - t3_loss: 280.1266 - t4_loss: 281.6297 - t5_loss: 1512.0052 - t6_loss: 283.3808 - first_layer_acc1: 0.0711 - second_layer_acc2: 0.0749 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2977.2073 - val_first_layer_loss: 3.1026 - val_second_layer_loss: 3.2449 - val_t1_loss: 290.2231 - val_t2_loss: 291.0401 - val_t3_loss: 282.6996 - val_t4_loss: 277.1565 - val_t5_loss: 1513.7084 - val_t6_loss: 316.0333 - val_first_layer_acc1: 0.1117 - val_second_layer_acc2: 0.0702 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2924.0774 - first_layer_loss: 3.1469 - second_layer_loss: 3.2195 - t1_loss: 281.4159 - t2_loss: 278.3330 - t3_loss: 280.1787 - t4_loss: 281.6821 - t5_loss: 1512.0052 - t6_loss: 284.0951 - first_layer_acc1: 0.0837 - second_layer_acc2: 0.0747 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2912.5808 - val_first_layer_loss: 3.1235 - val_second_layer_loss: 3.2125 - val_t1_loss: 274.5072 - val_t2_loss: 283.0743 - val_t3_loss: 278.3860 - val_t4_loss: 276.6976 - val_t5_loss: 1513.7084 - val_t6_loss: 279.8711 - val_first_layer_acc1: 0.0637 - val_second_layer_acc2: 0.0712 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2920.5603 - first_layer_loss: 3.0828 - second_layer_loss: 3.2079 - t1_loss: 281.4229 - t2_loss: 278.0097 - t3_loss: 279.0169 - t4_loss: 280.8789 - t5_loss: 1512.0052 - t6_loss: 282.9382 - first_layer_acc1: 0.1013 - second_layer_acc2: 0.0777 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2909.0015 - val_first_layer_loss: 2.9919 - val_second_layer_loss: 3.1695 - val_t1_loss: 278.6831 - val_t2_loss: 280.8998 - val_t3_loss: 277.9680 - val_t4_loss: 274.5100 - val_t5_loss: 1513.7084 - val_t6_loss: 277.0709 - val_first_layer_acc1: 0.1198 - val_second_layer_acc2: 0.0775 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2916.5540 - first_layer_loss: 3.0238 - second_layer_loss: 3.1952 - t1_loss: 279.6924 - t2_loss: 277.5518 - t3_loss: 277.9972 - t4_loss: 280.6938 - t5_loss: 1512.0052 - t6_loss: 282.3953 - first_layer_acc1: 0.1058 - second_layer_acc2: 0.0841 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2912.3564 - val_first_layer_loss: 2.9218 - val_second_layer_loss: 3.1520 - val_t1_loss: 272.0976 - val_t2_loss: 279.4365 - val_t3_loss: 289.3351 - val_t4_loss: 275.2474 - val_t5_loss: 1513.7084 - val_t6_loss: 276.4578 - val_first_layer_acc1: 0.1342 - val_second_layer_acc2: 0.1008 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2917.9722 - first_layer_loss: 2.9978 - second_layer_loss: 3.1729 - t1_loss: 280.8193 - t2_loss: 277.9326 - t3_loss: 278.6650 - t4_loss: 279.9031 - t5_loss: 1512.0052 - t6_loss: 282.4753 - first_layer_acc1: 0.1106 - second_layer_acc2: 0.0892 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2915.1724 - val_first_layer_loss: 2.9652 - val_second_layer_loss: 3.1277 - val_t1_loss: 283.5502 - val_t2_loss: 278.3809 - val_t3_loss: 276.7861 - val_t4_loss: 275.3239 - val_t5_loss: 1513.7084 - val_t6_loss: 281.3298 - val_first_layer_acc1: 0.0660 - val_second_layer_acc2: 0.1083 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 2913.0530 - first_layer_loss: 2.9272 - second_layer_loss: 3.1668 - t1_loss: 278.8539 - t2_loss: 277.0883 - t3_loss: 276.8302 - t4_loss: 280.2130 - t5_loss: 1512.0052 - t6_loss: 281.9687 - first_layer_acc1: 0.1300 - second_layer_acc2: 0.0935 - t1_accuracy: 0.0000e+00 - t2_accuracy: 0.0000e+00 - t3_accuracy: 0.0000e+00 - t4_accuracy: 0.0000e+00 - t5_accuracy: 0.0000e+00 - t6_accuracy: 0.0000e+00 - val_loss: 2899.6492 - val_first_layer_loss: 2.9003 - val_second_layer_loss: 3.1405 - val_t1_loss: 270.1436 - val_t2_loss: 277.9402 - val_t3_loss: 277.3088 - val_t4_loss: 275.1907 - val_t5_loss: 1513.7084 - val_t6_loss: 279.3162 - val_first_layer_acc1: 0.1583 - val_second_layer_acc2: 0.0983 - val_t1_accuracy: 0.0000e+00 - val_t2_accuracy: 0.0000e+00 - val_t3_accuracy: 0.0000e+00 - val_t4_accuracy: 0.0000e+00 - val_t5_accuracy: 0.0000e+00 - val_t6_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b3eb41a50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "    input_train,\n",
    "    {\n",
    "        \"first_layer\": output_train[\"First Layer\"],\n",
    "        \"second_layer\": output_train[\"Second Layer\"],\n",
    "        \"t1\": output_train[\"d1\"],\n",
    "        \"t2\": output_train[\"d2\"],\n",
    "        \"t3\": output_train[\"d3\"],\n",
    "        \"t4\": output_train[\"d4\"],\n",
    "        \"t5\": output_train[\"d5\"],\n",
    "        \"t6\": output_train[\"d6\"],\n",
    "    },\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        input_val,\n",
    "        {\n",
    "            \"first_layer\": output_val[\"First Layer\"],\n",
    "            \"second_layer\": output_val[\"Second Layer\"],\n",
    "            \"t1\": output_val[\"d1\"],\n",
    "            \"t2\": output_val[\"d2\"],\n",
    "            \"t3\": output_val[\"d3\"],\n",
    "            \"t4\": output_val[\"d4\"],\n",
    "            \"t5\": output_val[\"d5\"],\n",
    "            \"t6\": output_val[\"d6\"],\n",
    "        }\n",
    "    ),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "model predicted: ['Ag' 'Mg']\n",
      "actual combination: ['Mg' 'Ni']\n",
      "predicted_thicknesses: [35.21249, 36.12894, 34.560825, 35.76177, 0.0, 36.573948]\n",
      "actual thicknesses: [60 10 10 40 50 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clk/repos/l2-transfer-matrix-method/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(input_test) - 1)\n",
    "input_sample = input_test.iloc[[index]]\n",
    "output_sample = output_test.iloc[[index]]\n",
    "mat_predict = classifier.predict([input_sample])\n",
    "pred = label_encoder.inverse_transform([np.argmax(mat_predict[0]), np.argmax(mat_predict[1])])\n",
    "act = label_encoder.inverse_transform([output_sample[\"First Layer\"], output_sample[\"Second Layer\"]])\n",
    "\n",
    "print(f\"model predicted: {pred}\")\n",
    "print(f\"actual combination: {act}\")\n",
    "\n",
    "print(f\"predicted_thicknesses: {[elem[0][0] for elem in mat_predict[2:]]}\")\n",
    "print(f\"actual thicknesses: {output_sample.values[0][:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
